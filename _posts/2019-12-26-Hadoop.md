---
layout: entry
name: Apache Hadoop
webpage: https://hadoop.apache.org/
description: >
  Some computational problems can be solved very easily by breaking down the data into smaller buckets. For (a rather simple) example, say you're trying to find the largest number in a hundred million numbers. You can look through all of them one by one. Say you have a powerful computer than can look through a million numbers an hour, you'll need 100 hours or little over 4 days to do this.

  Now if you broke your data into a 100 pieces, and gave it to 100 computers, each computer will find its largest number in 1 hour and then you spend a few more seconds to find the largest among those hundred, you're pretty much done in about an hour.

  Problems like these are called embarrassingly parallel and the method of breaking it down(Mapping) into pieces and then joining the individual results to form a global result(Reducing) was described in a Google paper, and the technique is called MapReduce.

  Hadoop is an open source software that makes doing mapreduce type programming easier. You dont have to worry about installing the program on your 100 machines, breaking your initial data into pieces, copying it to all 100 machines, copying results over from 100 machines, etc. All the housekeeping is managed by Hadoop. Once you setup a hadoop cluster over the 100 machines, you can give it any program and data and it takes care of all the behind the scenes work and give you back the result.
source: https://www.reddit.com/r/explainlikeimfive/comments/1hkcuu/eli5_hadoop/cav5wyl?utm_source=share&utm_medium=web2x


tags:
- Distributed file system
- Big Data
- Apache
---
